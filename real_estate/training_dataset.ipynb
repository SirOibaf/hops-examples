{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset\n",
    "\n",
    "When building a model data scientists go trhough a phase of data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>8</td><td>application_1605601890461_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-247.us-west-2.compute.internal:8088/proxy/application_1605601890461_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-50.us-west-2.compute.internal:8042/node/containerlogs/container_e03_1605601890461_0006_01_000001/dataai__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import * \n",
    "import hsfs\n",
    "\n",
    "connection = hsfs.connection()\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step retrieve all the feature groups in which we are interested on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_fg = fs.get_feature_group(\"real_estate\", version=1)\n",
    "health_area_fg = fs.get_feature_group(\"health_area\", version=1)\n",
    "police_prct_fg = fs.get_feature_group(\"police_prct\", version=1)\n",
    "school_dist_fg = fs.get_feature_group(\"school_dist\", version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of the example, we are not interested in all the feature we have computed in the previous notebook. In the next cell we select only the specific ones, either by looking at the feature group metadata or by list them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_features = [f.name for f in real_estate_fg.features]\n",
    "real_estate_features.remove(\"property_id\")\n",
    "real_estate_features.remove(\"owner_type\")\n",
    "\n",
    "police_prct_features = ['police_prct_avg_sale_price', 'police_prct_avg_sale_price_owner', 'police_prct_sale_price_cat']\n",
    "health_area_features = ['health_area_avg_sale_price', 'health_area_avg_sale_price_owner', 'health_area_avg_sale_price_cat']\n",
    "school_dist_features = ['school_dist_avg_sale_price', 'school_dist_avg_sale_price_owner', 'school_dist_avg_sale_price_cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key components of building a training dataset is joining feature groups. In Hopsworks we use a Pandas-Like API that allow you to join several feature groups together. A basic example would be the following: \n",
    "\n",
    "```\n",
    "td_features = real_estate_fg.select(real_estate_features)\\\n",
    "              .join(health_area_fg.select(health_area_features))\\\n",
    "```\n",
    "\n",
    "In this case we select only the features we are interested from the real_estate feature group and from the health_area feature group. We let the query constructor in Hopsworks determine how to join the feature groups and which features to use as joining keys.\n",
    "\n",
    "\n",
    "The next cell is a bit more complex and involves us overwriting the joining keys with our user-provided ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_features = real_estate_fg.select(real_estate_features)\\\n",
    "              .join(health_area_fg.select(health_area_features), on=['health_area', 'owner_type', 'building_class'])\\\n",
    "              .join(police_prct_fg.select(police_prct_features), on=['police_prct', 'owner_type', 'building_class'])\\\n",
    "              .join(school_dist_fg.select(school_dist_features), on=['school_dist', 'owner_type', 'building_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining feature groups generate a SQL query that will be run on Spark. You can inspect the query for debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT `fg0`.`building_class`, `fg0`.`sale_price`, `fg0`.`is_owner_company`, `fg0`.`residential_units`, `fg0`.`age_at_sale`, `fg0`.`res_area`, `fg0`.`is_owner_organization`, `fg0`.`police_prct`, `fg0`.`garage_area`, `fg0`.`is_owner_private`, `fg0`.`is_large_residential`, `fg0`.`school_dist`, `fg0`.`health_area`, `fg0`.`is_single_unit`, `fg0`.`has_garage_area`, `fg1`.`health_area_avg_sale_price`, `fg1`.`health_area_avg_sale_price_owner`, `fg1`.`health_area_avg_sale_price_cat`, `fg2`.`police_prct_avg_sale_price`, `fg2`.`police_prct_avg_sale_price_owner`, `fg2`.`police_prct_sale_price_cat`, `fg3`.`school_dist_avg_sale_price`, `fg3`.`school_dist_avg_sale_price_owner`, `fg3`.`school_dist_avg_sale_price_cat`\n",
      "FROM `dataai_featurestore`.`real_estate_1` `fg0`\n",
      "INNER JOIN `dataai_featurestore`.`health_area_1` `fg1` ON `fg0`.`health_area` = `fg1`.`health_area` AND `fg0`.`owner_type` = `fg1`.`owner_type` AND `fg0`.`building_class` = `fg1`.`building_class`\n",
      "INNER JOIN `dataai_featurestore`.`police_prct_1` `fg2` ON `fg0`.`police_prct` = `fg2`.`police_prct` AND `fg0`.`owner_type` = `fg2`.`owner_type` AND `fg0`.`building_class` = `fg2`.`building_class`\n",
      "INNER JOIN `dataai_featurestore`.`school_dist_1` `fg3` ON `fg0`.`school_dist` = `fg3`.`school_dist` AND `fg0`.`owner_type` = `fg3`.`owner_type` AND `fg0`.`building_class` = `fg3`.`building_class`"
     ]
    }
   ],
   "source": [
    "print(td_features.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can use the query to create a training datasets. Creating a training dataset is similar to the feature group creation. For training datasets you can specify a label, or target feature and you can split the training datasets into different splits for training, testing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7f366a9f6950>"
     ]
    }
   ],
   "source": [
    "td = fs.create_training_dataset(\"real_estate_price\",\n",
    "                                version=1,\n",
    "                                data_format=\"csv\",\n",
    "                                label=['sale_price'],\n",
    "                                description=\"A dataset to train a real estate property value prediction\",\n",
    "                                statistics_config={'histograms': True, 'correlations': False},\n",
    "                                splits={'train': 0.7, 'test': 0.2, 'validate': 0.1})\n",
    "\n",
    "td.save(td_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}