{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #1EB382;font-weight: bold;\">Redshift Integration</h2>\n",
    "\n",
    "This notebooks guides through the ingestion of Redshift data in the Hopsworks feature store. To follow this notebook users should have an existing Redshift cluster, if not, they can follow the AWS [documentation](https://docs.aws.amazon.com/ses/latest/DeveloperGuide/event-publishing-redshift-cluster.html).\n",
    "\n",
    "The data for this tutorial is available in CSV format [here]()\n",
    "Users should create the following table in Redshift\n",
    "```sql\n",
    "CREATE TABLE telco(\n",
    "    customer_id varchar(200),\n",
    "    gender varchar(200),\n",
    "    senior_citizen integer,\n",
    "    partner varchar(200),\n",
    "    dependents varchar(200),\n",
    "    tenure integer,\n",
    "    phone_service varchar(200),\n",
    "    multiple_lines varchar(200),\n",
    "    internet_service varchar(200),\n",
    "    online_security varchar(200),\n",
    "    online_backup varchar(200),\n",
    "    device_protection varchar(200),\n",
    "    tech_support varchar(200),\n",
    "    streaming_tv varchar(200),\n",
    "    streaming_movies varchar(200),\n",
    "    contract varchar(200),\n",
    "    paperless_billing varchar(200),\n",
    "    payment_method varchar(200),\n",
    "    monthly_charges double precision,\n",
    "    total_charges varchar(200),\n",
    "    churn varchar(200)\n",
    ")\n",
    "```\n",
    "\n",
    "and populate the table using the copy command:\n",
    "```sql\n",
    "COPY telco\n",
    "FROM 's3://bucket/telco_customer_churn.csv'\n",
    "IAM_ROLE 'arn:aws:iam::xxxxxxxxx:role/role_name'\n",
    "FORMAT as CSV\n",
    "FILLRECORD\n",
    "```\n",
    "\n",
    "Once the data has been imported into Redshift, we can start ingesting it into the Hopsworks Feature Store. \n",
    "\n",
    "<h3 style=\"color: #1EB382;font-weight: bold;\">Storage Connector</h3>\n",
    "\n",
    "The first step to be able to ingest Redshift data in the feature store is to configure a storage connector. \n",
    "Hopsworks storage connectors are meant as a centralized place to store and manage configurations to read and write to external system. \n",
    "\n",
    "The Redshift connector requires you to specify the following properties. Most of them are available in the properties area of your cluster in the Redshift UI. \n",
    "\n",
    "- Cluster identifier: The name of the cluster\n",
    "\n",
    "- Database driver: You can use the default JDBC Redshift Driver `com.amazon.redshift.jdbc42.Driver` (More on this later)\n",
    "\n",
    "- Database endpoint: The endpoint for the database. Should be in the format of `[UUID].eu-west-1.redshift.amazonaws.com`\n",
    "\n",
    "- Database name: The name of the database to query\n",
    "\n",
    "- Database port: The port of the cluster. Defaults to 5349\n",
    "\n",
    "\n",
    "There are two options to authenticate with the Redshift cluster. The first option is to configure username and password. The password is stored in the secret store and made available to all the member of the project.\n",
    "\n",
    "Alternatively a IAM role can be configured. When configured the HSFS library will use the IAM role to acquire a temporary credential to authenticate the specified user.\n",
    "\n",
    "With regards to the database driver, the library to interact with Redshift *is not* part of the standard Hopsworks distribution and needs to be provided by the user. The library can be downloaded here: https://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html#download-jdbc-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>36</td><td>application_1608750159023_0006</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-35-74.eu-west-1.compute.internal:8088/proxy/application_1608750159023_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-35-74.eu-west-1.compute.internal:8042/node/containerlogs/container_e02_1608750159023_0006_01_000001/demo_fs_meb10000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "import com.logicalclocks.hsfs._\n",
      "import scala.collection.JavaConversions._\n",
      "import collection.JavaConverters._\n",
      "import org.apache.spark.sql.{DataFrame, Row}\n",
      "import org.apache.spark.sql.types._\n",
      "connection: com.logicalclocks.hsfs.HopsworksConnection = com.logicalclocks.hsfs.HopsworksConnection@5b16ea87\n",
      "fs: com.logicalclocks.hsfs.FeatureStore = FeatureStore{id=67, name='demo_fs_meb10000_featurestore', projectId=119, featureGroupApi=com.logicalclocks.hsfs.metadata.FeatureGroupApi@3b06db70}\n"
     ]
    }
   ],
   "source": [
    "import com.logicalclocks.hsfs._\n",
    "import scala.collection.JavaConversions._\n",
    "import collection.JavaConverters._\n",
    "\n",
    "import org.apache.spark.sql.{ DataFrame, Row }\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val connection = HopsworksConnection.builder().build();\n",
    "val fs = connection.getFeatureStore();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #1EB382;font-weight: bold;\">On-Demand Feature Group</h3>\n",
    "\n",
    "To be able to read data from the Redshift table, an on-demand feature group needs to be defined.\n",
    "In Hopsworks on-demand feature groups allow users to register features stored on external systems (Redshift, RDS, S3) with the Hopsworks feature store. \n",
    "\n",
    "Registering external features with the Hopsworks feature store has several advantages. The first one is *provenance*, from the Hopsworks Web UI users are able to track which features are stored on which external systems and how they are computed. \n",
    "Additionally HSFS (the Python/Scala library used to interact with the feature store) provides the same APIs for on-demand feature groups as for feature groups available within Hopsworks itself (Cached feature groups). Thus abstracting the complexity of configuring data ingestion.\n",
    "\n",
    "An on-demand feature group can be defined as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redshiftConn: com.logicalclocks.hsfs.StorageConnector = StorageConnector(id=1025, name=telco_redshift_cluster, accessKey=null, secretKey=null, serverEncryptionAlgorithm=null, serverEncryptionKey=null, bucket=null, clusterIdentifier=telco-redshift-cluster, databaseDriver=com.amazon.redshift.jdbc42.Driver, databaseEndpoint=cxwh6weoo4ae.eu-west-1.redshift.amazonaws.com, databaseName=dev, databasePort=5439, tableName=null, databaseUserName=awsuser, autoCreate=null, databaseGroup=null, expiration=null, databasePassword=Fabio123, sessionToken=null, connectionString=null, arguments=, storageConnectorType=REDSHIFT)\n"
     ]
    }
   ],
   "source": [
    "// Retrieve the storage connector defined before\n",
    "val redshiftConn = fs.getStorageConnector(\"telco_redshift_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "telcoOnDmd: com.logicalclocks.hsfs.OnDemandFeatureGroup = com.logicalclocks.hsfs.OnDemandFeatureGroup@4d876d28\n"
     ]
    }
   ],
   "source": [
    "val telcoOnDmd = (fs.createOnDemandFeatureGroup()\n",
    "                    .name(\"telco_redshift_scala\")\n",
    "                    .version(2)\n",
    "                    .query(\"select * from telco\")\n",
    "                    .description(\"On-demand feature group for telecom customer data\")\n",
    "                    .storageConnector(redshiftConn)\n",
    "                    .statisticsEnabled(true)\n",
    "                    .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "telcoOnDmd.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #1EB382;font-weight: bold;\">Feature Engineering</h3>\n",
    "\n",
    "On-demand feature groups can be used directly as a source for creating training datasets. This is often the case if a company is migrating to Hopsworks and there are already feature engineering pipelines in production writing data to Redshift.\n",
    "\n",
    "This flexibility provided by Hopsworks allows users to hit the ground running from day 1, without having to rewrite their pipelines to take advantage of the benefits the Hopsworks feature store provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-------------+-------------+-----+\n",
      "|customer_id|internet_service|phone_service|total_charges|churn|\n",
      "+-----------+----------------+-------------+-------------+-----+\n",
      "| 7590-VHVEG|             DSL|           No|        29.85|   No|\n",
      "| 5575-GNVDE|             DSL|          Yes|       1889.5|   No|\n",
      "| 3668-QPYBK|             DSL|          Yes|       108.15|  Yes|\n",
      "| 7795-CFOCW|             DSL|           No|      1840.75|   No|\n",
      "| 9237-HQITU|     Fiber optic|          Yes|       151.65|  Yes|\n",
      "+-----------+----------------+-------------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "telcoOnDmd.select(Seq(\"customer_id\", \"internet_service\", \"phone_service\", \"total_charges\", \"churn\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On-demand feature groups can also be joined with cached feature groups to create training datasets. This document (https://docs.hopsworks.ai/generated/query_vs_dataframe/) explains in details how the HSFS joining APIs work and how they can be used to create training datasets.\n",
    "\n",
    "If, however, Redshift contains raw data that needs to be feature engineered, users can invoke the HSFS API and retrieve a Spark DataFrame backed by the Redshift table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [customer_id: string, gender: string ... 19 more fields]\n"
     ]
    }
   ],
   "source": [
    "var sparkDf = telcoOnDmd.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.StringIndexer\n",
      "import org.apache.spark.ml.{Pipeline, PipelineModel}\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categoricalColumns: Seq[String] = List(gender, senior_citizen, partner, dependents, phone_service, multiple_lines, internet_service, online_security, online_backup, device_protection, tech_support, streaming_tv, streaming_movies, contract, paperless_billing, payment_method, churn)\n",
      "sparkDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [customer_id: string, gender: string ... 19 more fields]\n",
      "stages: List[org.apache.spark.ml.feature.StringIndexer] = List()\n",
      "outputCols: List[(String, String)] = List((customer_id,customer_id))\n",
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_3f1d5b001461\n",
      "dataset: org.apache.spark.sql.DataFrame = [customer_id: string, gender: string ... 36 more fields]\n",
      "telcoFgDf: org.apache.spark.sql.DataFrame = [churn_index: string, payment_method_index: string ... 16 more fields]\n"
     ]
    }
   ],
   "source": [
    "val categoricalColumns = Seq(\"gender\", \"senior_citizen\",\"partner\",\"dependents\",\"phone_service\",\"multiple_lines\",\n",
    "                      \"internet_service\", \"online_security\", \"online_backup\", \"device_protection\", \"tech_support\",\n",
    "                      \"streaming_tv\", \"streaming_movies\", \"contract\", \"paperless_billing\", \"payment_method\", \"churn\")\n",
    "\n",
    "sparkDf = sparkDf.withColumn(\"total_charges\", $\"total_charges\".cast(DoubleType)).na.fill(0)\n",
    "\n",
    "var stages = List[StringIndexer]() // stages in our Pipeline\n",
    "var outputCols = List((\"customer_id\", \"customer_id\"))\n",
    "\n",
    "for (categoricalCol <- categoricalColumns) {\n",
    "    // Category Indexing with StringIndexer\n",
    "    val outputCol = categoricalCol + \"_index\"\n",
    "    val stringIndexer = new StringIndexer()\n",
    "    stringIndexer.setInputCol(categoricalCol)\n",
    "    stringIndexer.setOutputCol(outputCol)\n",
    "    \n",
    "    stages = stringIndexer :: stages\n",
    "    outputCols = (categoricalCol, outputCol) :: outputCols\n",
    "}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(stages.toArray)\n",
    "val dataset = pipeline.fit(sparkDf).transform(sparkDf)\n",
    "val telcoFgDf = dataset.selectExpr(outputCols.map(oc => oc._1  + \" as \" + oc._2):_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------------------+--------------+----------------------+------------------+------------------+-----------------------+-------------------+---------------------+----------------------+--------------------+-------------------+----------------+-------------+--------------------+------------+-----------+\n",
      "|churn_index|payment_method_index|paperless_billing_index|contract_index|streaming_movies_index|streaming_tv_index|tech_support_index|device_protection_index|online_backup_index|online_security_index|internet_service_index|multiple_lines_index|phone_service_index|dependents_index|partner_index|senior_citizen_index|gender_index|customer_id|\n",
      "+-----------+--------------------+-----------------------+--------------+----------------------+------------------+------------------+-----------------------+-------------------+---------------------+----------------------+--------------------+-------------------+----------------+-------------+--------------------+------------+-----------+\n",
      "|         No|    Electronic check|                    Yes|Month-to-month|                    No|                No|                No|                     No|                Yes|                   No|                   DSL|    No phone service|                 No|              No|          Yes|                   0|      Female| 7590-VHVEG|\n",
      "|         No|        Mailed check|                     No|      One year|                    No|                No|                No|                    Yes|                 No|                  Yes|                   DSL|                  No|                Yes|              No|           No|                   0|        Male| 5575-GNVDE|\n",
      "|        Yes|        Mailed check|                    Yes|Month-to-month|                    No|                No|                No|                     No|                Yes|                  Yes|                   DSL|                  No|                Yes|              No|           No|                   0|        Male| 3668-QPYBK|\n",
      "|         No|Bank transfer (au...|                     No|      One year|                    No|                No|               Yes|                    Yes|                 No|                  Yes|                   DSL|    No phone service|                 No|              No|           No|                   0|        Male| 7795-CFOCW|\n",
      "|        Yes|    Electronic check|                    Yes|Month-to-month|                    No|                No|                No|                     No|                 No|                   No|           Fiber optic|                  No|                Yes|              No|           No|                   0|      Female| 9237-HQITU|\n",
      "+-----------+--------------------+-----------------------+--------------+----------------------+------------------+------------------+-----------------------+-------------------+---------------------+----------------------+--------------------+-------------------+----------------+-------------+--------------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "telcoFgDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #1EB382;font-weight: bold;\">Derived Feature Group</h3>\n",
    "\n",
    "Finally users can save the output of their transformations in the feature store as a derived cached feature group.\n",
    "\n",
    "Storing feature groups as cached feature groups within Hopsworks provides several benefits. First it allows users to leverage HUDI for upserts and time travel capabilities. As new data is ingested, new commits are tracked by Hopsworks allowing users to see what has changed over time.\n",
    "Similarly to on-demand feature groups, at each commit, statistics are computed and tracked in Hopsworks, allowing users to understand how the data changes.\n",
    "\n",
    "Cached feature groups can also be made available online (`online_enabled=True`) thus enabling low latency requests to augment inference vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "telcoFg: com.logicalclocks.hsfs.FeatureGroup = com.logicalclocks.hsfs.FeatureGroup@6927f5b8\n"
     ]
    }
   ],
   "source": [
    "val telcoFg = (fs.createFeatureGroup()\n",
    "                 .name(\"telco_customer_features\")\n",
    "                 .version(2)\n",
    "                 .description(\"Telecom customer features\")\n",
    "                 .onlineEnabled(true)\n",
    "                 .timeTravelFormat(TimeTravelFormat.HUDI)\n",
    "                 .primaryKeys(Seq(\"customer_id\"))\n",
    "                 .statisticsEnabled(true)\n",
    "                 .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "java.io.IOException: Error: 400{\"type\":\"restApiJsonResponse\",\"errorCode\":270121,\"errorMsg\":\"Primary key and partition key are required when using Hudi time travel format\"}\n",
      "  at com.logicalclocks.hsfs.metadata.AuthorizationHandler.handleResponse(AuthorizationHandler.java:45)\n",
      "  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:223)\n",
      "  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:191)\n",
      "  at com.logicalclocks.hsfs.metadata.HopsworksInternalClient.handleRequest(HopsworksInternalClient.java:148)\n",
      "  at com.logicalclocks.hsfs.metadata.HopsworksClient.handleRequest(HopsworksClient.java:151)\n",
      "  at com.logicalclocks.hsfs.metadata.FeatureGroupApi.saveInternal(FeatureGroupApi.java:125)\n",
      "  at com.logicalclocks.hsfs.metadata.FeatureGroupApi.save(FeatureGroupApi.java:105)\n",
      "  at com.logicalclocks.hsfs.engine.FeatureGroupEngine.saveFeatureGroup(FeatureGroupEngine.java:90)\n",
      "  at com.logicalclocks.hsfs.FeatureGroup.save(FeatureGroup.java:178)\n",
      "  at com.logicalclocks.hsfs.FeatureGroup.save(FeatureGroup.java:173)\n",
      "  ... 65 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "telcoFg.save(telcoFgDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}