{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HudiOnHops - PySpark\n",
    "\n",
    "This notebook is meant as a continuation of [HudiOnHops - Scala](https://github.com/logicalclocks/hops-examples/blob/master/notebooks/featurestore/hudi/HudiOnHops.ipynb). Check out that notebook first to get an introduction of what is Apache Hudi and how you can leverage it for dataset upserts and time travel.\n",
    "\n",
    "This notebook will walk you through how to achieve the same results in PySpark and how to use the feature store Python API to create and update Hudi feature groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>18</td><td>application_1574727804458_0028</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8088/proxy/application_1574727804458_0028/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e01_1574727804458_0028_01_000001/demo_featurestore_admin000__meb10000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7fba7607ddd8>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For the feature store we are going to leverage Hudi's capability of synchronizing Hudi datasets with Apache Hive external tables. To do so, we need to do some setup.\n",
    "First we need to build the JDBC string which Hudi is going to use to synchronize the dataset and its partitions with Hive.\n",
    "\n",
    "Please note that in the following example you should replace the IP with the IP/hostname of your instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import featurestore, hdfs, tls\n",
    "jdbc_conn = (\"jdbc:hive2://10.0.2.15:9085/{};auth=noSasl;ssl=true;twoWay=true;\" + \\\n",
    "            \"sslTrustStore=t_certificate;trustStorePassword={};\" + \\\n",
    "            \"sslKeyStore=k_certificate;keyStorePassword={}\").format(featurestore.project_featurestore(), tls.get_key_store_pwd(), tls.get_key_store_pwd())\n",
    "project_name = hdfs.project_name()\n",
    "feature_group_name = \"hudi_fg_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk insert\n",
    "\n",
    "In this step we are going to generate a Spark dataframe containing some sample data and persist it as a Hudi dataset. We are then going to register this dataset as feature group with the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+-------+\n",
      "| id|      date| value|country|\n",
      "+---+----------+------+-------+\n",
      "|  1|2019-02-30|0.4151| Sweden|\n",
      "|  2|2019-05-01|1.2151|Ireland|\n",
      "|  3|2019-08-06|0.2151|Belgium|\n",
      "|  4|2019-08-06|0.8151| Russia|\n",
      "+---+----------+------+-------+"
     ]
    }
   ],
   "source": [
    "bulkInsertData = [\n",
    "    (1, \"2019-02-30\", 0.4151, \"Sweden\"),\n",
    "    (2, \"2019-05-01\", 1.2151, \"Ireland\"),\n",
    "    (3, \"2019-08-06\", 0.2151, \"Belgium\"),\n",
    "    (4, \"2019-08-06\", 0.8151, \"Russia\")\n",
    "]\n",
    "\n",
    "columns = ['id', 'date', 'value', 'country']\n",
    "bulkInsertDf = spark.createDataFrame(bulkInsertData, columns)\n",
    "bulkInsertDf.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the meaning of the options specified in the other notebook. As a reminder, Hudi is going to detect duplicates entries based on the `HoodieKey` which is the combination of `hoodie.datasource.write.recordkey.field` and `hoodie.datasource.write.partitionpath.field`. In future upserts, duplicated entries will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulkInsertDf.write.format(\"org.apache.hudi\") \\\n",
    "            .option(\"hoodie.table.name\", feature_group_name) \\\n",
    "            .option(\"hoodie.datasource.write.storage.type\", \"COPY_ON_WRITE\") \\\n",
    "            .option(\"hoodie.datasource.write.operation\", \"bulk_insert\") \\\n",
    "            .option(\"hoodie.datasource.write.recordkey.field\",\"id\") \\\n",
    "            .option(\"hoodie.datasource.write.partitionpath.field\", \"date\") \\\n",
    "            .option(\"hoodie.datasource.write.precombine.field\", \"value\") \\\n",
    "            .option(\"hoodie.datasource.hive_sync.enable\", \"true\") \\\n",
    "            .option(\"hoodie.datasource.hive_sync.table\", feature_group_name) \\\n",
    "            .option(\"hoodie.datasource.hive_sync.database\", featurestore.project_featurestore()) \\\n",
    "            .option(\"hoodie.datasource.hive_sync.jdbcurl\", jdbc_conn) \\\n",
    "            .option(\"hoodie.datasource.hive_sync.partition_fields\", \"date\") \\\n",
    "            .option(\"hoodie.datasource.hive_sync.partition_extractor_class\", \"org.apache.hudi.hive.MultiPartKeysValueExtractor\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(\"hdfs:///Projects/{}/Resources/{}\".format(project_name, feature_group_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step we are going to register the Hudi dataset created above with the feature store and compute the statistics for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synchronizing Hive Table: hudi_fg with Feature Store: demo_featurestore_admin000_featurestore\n",
      "Hive Table: hudi_fg was successfully synchronized with Feature Store: demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "featurestore.sync_hive_table_with_featurestore(\"hudi_fg\", featuregroup_version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore against offline feature store\n",
      "SQL string for the query created successfully\n",
      "Running sql: SELECT * FROM hudi_fg_1 against offline feature store\n",
      "computing descriptive statistics for : hudi_fg, version: 1\n",
      "computing feature correlation for: hudi_fg, version: 1\n",
      "computing feature histograms for: hudi_fg, version: 1\n",
      "computing cluster analysis for: hudi_fg, version: 1"
     ]
    }
   ],
   "source": [
    "featurestore.update_featuregroup_stats(\"hudi_fg\", featuregroup_version=1, descriptive_statistics=True,feature_correlation=True, \\\n",
    "                                       feature_histograms=True, cluster_analysis=True, stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we simulate the arrival of new data. Some data is new (insert) other data has been modified (update). We are going to append it to the feature group created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+----------------+\n",
      "| id|      date| value|         country|\n",
      "+---+----------+------+----------------+\n",
      "|  5|2019-02-30|0.7921|Northern Ireland|\n",
      "|  1|2019-02-30| 1.151|          Norway|\n",
      "|  3|2019-08-06| 0.999|         Belgium|\n",
      "|  6|2019-08-01|0.0151|          France|\n",
      "+---+----------+------+----------------+"
     ]
    }
   ],
   "source": [
    "upsertData = [\n",
    "    (5, \"2019-02-30\", 0.7921, \"Northern Ireland\"), # Insert\n",
    "    (1, \"2019-02-30\", 1.151, \"Norway\"), # Update\n",
    "    (3, \"2019-08-06\", 0.999, \"Belgium\"), # Update\n",
    "    (6, \"2019-08-01\", 0.0151, \"France\") # Insert\n",
    "]\n",
    "\n",
    "upsertDf = spark.createDataFrame(upsertData, columns)\n",
    "upsertDf.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsertDf.write.format(\"org.apache.hudi\") \\\n",
    "            .option(\"hoodie.table.name\", feature_group_name) \\\n",
    "            .option(\"hoodie.datasource.write.storage.type\", \"COPY_ON_WRITE\") \\\n",
    "            .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "            .option(\"hoodie.datasource.write.recordkey.field\",\"id\") \\\n",
    "            .option(\"hoodie.datasource.write.partitionpath.field\", \"date\") \\\n",
    "            .option(\"hoodie.datasource.write.precombine.field\", \"value\") \\\n",
    "            .option(\"hoodie.datasource.hive_sync.enable\", \"true\") \\\n",
    "            .option(\"hoodie.datasource.hive_sync.table\", feature_group_name) \\\n",
    "            .option(\"hoodie.datasource.hive_sync.database\", featurestore.project_featurestore()) \\\n",
    "            .option(\"hoodie.datasource.hive_sync.jdbcurl\", jdbc_conn) \\\n",
    "            .option(\"hoodie.datasource.hive_sync.partition_fields\", \"date\") \\\n",
    "            .option(\"hoodie.datasource.hive_sync.partition_extractor_class\", \"org.apache.hudi.hive.MultiPartKeysValueExtractor\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save(\"hdfs:///Projects/{}/Resources/{}\".format(project_name, feature_group_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also refresh the statistics to consider the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore against offline feature store\n",
      "SQL string for the query created successfully\n",
      "Running sql: SELECT * FROM hudi_fg_1 against offline feature store\n",
      "computing descriptive statistics for : hudi_fg, version: 1\n",
      "computing feature correlation for: hudi_fg, version: 1\n",
      "computing feature histograms for: hudi_fg, version: 1\n",
      "computing cluster analysis for: hudi_fg, version: 1"
     ]
    }
   ],
   "source": [
    "featurestore.update_featuregroup_stats(\"hudi_fg\", featuregroup_version=1, descriptive_statistics=True,feature_correlation=True, \\\n",
    "                                       feature_histograms=True, cluster_analysis=True, stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the content of the feature group\n",
    "\n",
    "Using the Apache Hudi API you will be able to read the latest version of the feature group content or travel back in time and generate a training dataset from a previous version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"org.apache.hudi\")\\\n",
    "          .load(\"hdfs:///Projects/demo_featurestore_admin000/Resources/hudi_fg_1/*/*\")\n",
    "df.registerTempTable(\"snapshot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "| _hoodie_commit_time|   string|   null|\n",
      "|_hoodie_commit_seqno|   string|   null|\n",
      "|  _hoodie_record_key|   string|   null|\n",
      "|_hoodie_partition...|   string|   null|\n",
      "|   _hoodie_file_name|   string|   null|\n",
      "|                  id|   bigint|   null|\n",
      "|                date|   string|   null|\n",
      "|               value|   double|   null|\n",
      "|             country|   string|   null|\n",
      "+--------------------+---------+-------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe snapshot\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the table contains some Hudi internal fields used to track commits and allow time travel. We should remove them before generating the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = spark.sql(\"select id, date, value, country from snapshot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+----------------+\n",
      "| id|      date| value|         country|\n",
      "+---+----------+------+----------------+\n",
      "|  1|2019-02-30| 1.151|          Norway|\n",
      "|  5|2019-02-30|0.7921|Northern Ireland|\n",
      "|  6|2019-08-01|0.0151|          France|\n",
      "|  3|2019-08-06| 0.999|         Belgium|\n",
      "|  2|2019-05-01|1.2151|         Ireland|\n",
      "|  4|2019-08-06|0.8151|          Russia|\n",
      "+---+----------+------+----------------+"
     ]
    }
   ],
   "source": [
    "clean_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset created successfully"
     ]
    }
   ],
   "source": [
    "featurestore.create_training_dataset(clean_df, \"HudiTrainingDs\", description=\"\", \\\n",
    "                                     featurestore=featurestore.project_featurestore(), data_format=\"csv\", \\\n",
    "                                     training_dataset_version=1, descriptive_statistics=False, feature_correlation=False, \\\n",
    "                                     feature_histograms=False, cluster_analysis=False, stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hudi in PySpark does have an easy way of accessing the timeline to retrieve all the commits. However, you can see the list of commits by querying the dataset as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|_hoodie_commit_time|\n",
      "+-------------------+\n",
      "|     20191126204338|\n",
      "|     20191126204338|\n",
      "|     20191126204338|\n",
      "|     20191126204338|\n",
      "|     20191126203237|\n",
      "|     20191126203237|\n",
      "+-------------------+"
     ]
    }
   ],
   "source": [
    "tt = spark.read.format(\"org.apache.hudi\") \\\n",
    "          .load(\"hdfs:///Projects/demo_featurestore_admin000/Resources/hudi_fg_1/*/*\")\n",
    "tt.registerTempTable(\"tt\")\n",
    "spark.sql(\"select _hoodie_commit_time from tt\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The options `hoodie.datasource.read.begin.instanttime` and `hoodie.datasource.read.end.instanttime` allow you to specify for which time frame to retrieve the data. The values should be timestamps, but they don't have to correspond to specific commit times. In the example below are retrieving the entries that changed from `0`(the beginning) and `20191126204000` (the last commit included in the range was at `20191126203237`).\n",
    "\n",
    "Please note that if you re-run the notebook, the timestamps will differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "incrementalDf = spark.read.format(\"org.apache.hudi\") \\\n",
    "                     .option(\"hoodie.datasource.view.type\", \"incremental\") \\\n",
    "                     .option(\"hoodie.datasource.read.begin.instanttime\", \"0\") \\\n",
    "                     .option(\"hoodie.datasource.read.end.instanttime\", \"20191126204000\") \\\n",
    "                     .load(\"hdfs:///Projects/demo_featurestore_admin000/Resources/hudi_fg_1\")\n",
    "incrementalDf.registerTempTable(\"incremental_df\")\n",
    "incrementalDf = spark.sql(\"select id, value, date, country from incremental_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can generate a training dataset out of this dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset created successfully"
     ]
    }
   ],
   "source": [
    "featurestore.create_training_dataset(incrementalDf, \"HudiTrainingDs_timetravel\", description=\"\", \\\n",
    "                                     featurestore=featurestore.project_featurestore(), data_format=\"csv\", \\\n",
    "                                     training_dataset_version=1, descriptive_statistics=False, feature_correlation=False, \\\n",
    "                                     feature_histograms=False, cluster_analysis=False, stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletes\n",
    "\n",
    "If you need to delete some records from a Hudi dataset you have 2 options:\n",
    "- You do an upsert of the records you want to delete with all the fields set to NULL\n",
    "- You issue a delete of the records. You can do so by setting the option: `.option(\"hoodie.datasource.write.payload.class\", \"org.apache.hudi.EmptyHoodieRecordPayload\")`\n",
    "\n",
    "The cells below illustrate the second option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "russiaDf = spark.sql('SELECT * FROM snapshot WHERE country=\"Russia\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+----------+------+-------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|      date| value|country|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+----------+------+-------+\n",
      "|     20191126203237|  20191126203237_3_8|                 4|            2019-08-06|971eed02-334c-4e5...|  4|2019-08-06|0.8151| Russia|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+----------+------+-------+"
     ]
    }
   ],
   "source": [
    "russiaDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "russiaDf.write.format(\"org.apache.hudi\") \\\n",
    "            .option(\"hoodie.table.name\", feature_group_name) \\\n",
    "            .option(\"hoodie.datasource.write.storage.type\", \"COPY_ON_WRITE\") \\\n",
    "            .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "            .option(\"hoodie.datasource.write.recordkey.field\",\"id\") \\\n",
    "            .option(\"hoodie.datasource.write.partitionpath.field\", \"date\") \\\n",
    "            .option(\"hoodie.datasource.write.precombine.field\", \"value\") \\\n",
    "            .option(\"hoodie.datasource.hive_sync.enable\", \"true\") \\\n",
    "            .option(\"hoodie.datasource.hive_sync.table\", feature_group_name) \\\n",
    "            .option(\"hoodie.datasource.hive_sync.database\", featurestore.project_featurestore()) \\\n",
    "            .option(\"hoodie.datasource.hive_sync.jdbcurl\", jdbc_conn) \\\n",
    "            .option(\"hoodie.datasource.hive_sync.partition_fields\", \"date\") \\\n",
    "            .option(\"hoodie.datasource.hive_sync.partition_extractor_class\", \"org.apache.hudi.hive.MultiPartKeysValueExtractor\") \\\n",
    "            .option(\"hoodie.datasource.write.payload.class\", \"org.apache.hudi.EmptyHoodieRecordPayload\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save(\"hdfs:///Projects/{}/Resources/{}\".format(project_name, feature_group_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we get the current snapshot of the dataset, we see that the record was removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+----------+------+----------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|      date| value|         country|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+----------+------+----------------+\n",
      "|     20191126204338| 20191126204338_1_10|                 1|            2019-02-30|8a90bc02-03fe-459...|  1|2019-02-30| 1.151|          Norway|\n",
      "|     20191126204338| 20191126204338_1_11|                 5|            2019-02-30|8a90bc02-03fe-459...|  5|2019-02-30|0.7921|Northern Ireland|\n",
      "|     20191126204338| 20191126204338_2_12|                 6|            2019-08-01|5652d001-3d1d-48b...|  6|2019-08-01|0.0151|          France|\n",
      "|     20191126204338|  20191126204338_0_9|                 3|            2019-08-06|480b1cd9-4644-42c...|  3|2019-08-06| 0.999|         Belgium|\n",
      "|     20191126203237|  20191126203237_1_6|                 2|            2019-05-01|919dd34c-1ba7-4e1...|  2|2019-05-01|1.2151|         Ireland|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+----------+------+----------------+"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"org.apache.hudi\")\\\n",
    "          .load(\"hdfs:///Projects/demo_featurestore_admin000/Resources/hudi_fg_1/*/*\")\n",
    "df.registerTempTable(\"snapshot\")\n",
    "spark.sql(\"SELECT * FROM snapshot\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can still travel back in time and retrieve the record if you need to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+----------+------+-------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|      date| value|country|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+----------+------+-------+\n",
      "|     20191126203237|  20191126203237_1_6|                 2|            2019-05-01|919dd34c-1ba7-4e1...|  2|2019-05-01|1.2151|Ireland|\n",
      "|     20191126203237|  20191126203237_2_7|                 3|            2019-08-06|480b1cd9-4644-42c...|  3|2019-08-06|0.2151|Belgium|\n",
      "|     20191126203237|  20191126203237_0_5|                 1|            2019-02-30|8a90bc02-03fe-459...|  1|2019-02-30|0.4151| Sweden|\n",
      "|     20191126203237|  20191126203237_3_8|                 4|            2019-08-06|971eed02-334c-4e5...|  4|2019-08-06|0.8151| Russia|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+----------+------+-------+"
     ]
    }
   ],
   "source": [
    "incrementalDf = spark.read.format(\"org.apache.hudi\") \\\n",
    "                     .option(\"hoodie.datasource.view.type\", \"incremental\") \\\n",
    "                     .option(\"hoodie.datasource.read.begin.instanttime\", \"0\") \\\n",
    "                     .option(\"hoodie.datasource.read.end.instanttime\", \"20191126204000\") \\\n",
    "                     .load(\"hdfs:///Projects/demo_featurestore_admin000/Resources/hudi_fg_1\")\n",
    "incrementalDf.registerTempTable(\"incremental_df\")\n",
    "spark.sql(\"SELECT * FROM incremental_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More documentation, including documentation on how to compact the commits and reduce the storage consumption, is available [here](https://hudi.apache.org/configurations.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}